
// server/index.js
import 'dotenv/config';
import express from 'express';
import helmet from 'helmet';
import cors from 'cors';
import morgan from 'morgan';
import rateLimit from 'express-rate-limit';
import OpenAI from 'openai';
import path from 'node:path';
import { fileURLToPath } from 'node:url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const app = express();
const PORT = process.env.PORT || 3000;
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// --- middleware ---
app.use(helmet());
app.use(express.json({ limit: '1mb' }));

const corsOrigin = process.env.CORS_ORIGIN || '*';
app.use(cors({ origin: corsOrigin }));
app.use(morgan('dev'));

const limiter = rateLimit({
  windowMs: 60 * 1000,
  max: 60,
  standardHeaders: true,
  legacyHeaders: false,
});
app.use('/api/', limiter);

// serve static web (optional; you can also use nginx to serve /web contents)
app.use('/', express.static(path.join(__dirname, '../web')));

app.get('/api/health', (_req, res) => res.json({ ok: true }));

// Non-streaming chat endpoint
app.post('/api/chat', async (req, res) => {
  try {
    const { messages = [], model = process.env.OPENAI_MODEL || 'gpt-4o-mini', system = 'You are a helpful assistant.' } = req.body || {};
    const input = [{ role: 'system', content: system }, ...messages];
    const r = await client.responses.create({ model, input });
    res.json({ text: r.output_text });
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: err.message || 'OpenAI error' });
  }
});

// Streaming chat endpoint (NDJSON chunks)
app.post('/api/chat-stream', async (req, res) => {
  try {
    const { messages = [], model = process.env.OPENAI_MODEL || 'gpt-4o-mini', system = 'You are a helpful assistant.' } = req.body || {};
    const input = [{ role: 'system', content: system }, ...messages];

    res.setHeader('Content-Type', 'application/x-ndjson; charset=utf-8');
    res.setHeader('Cache-Control', 'no-cache, no-transform');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no'); // for nginx

    const stream = await client.responses.create({
      model,
      input,
      stream: true,
    });

    for await (const event of stream) {
      if (event.type === 'response.output_text.delta' && event.delta) {
        res.write(JSON.stringify({ delta: event.delta }) + "\n");
      } else if (event.type === 'response.completed') {
        res.write(JSON.stringify({ done: true }) + "\n");
        res.end();
      } else if (event.type === 'error') {
        res.write(JSON.stringify({ error: event.error || 'stream error' }) + "\n");
        res.end();
      }
    }
  } catch (err) {
    console.error('stream error', err);
    try { res.write(JSON.stringify({ error: err.message || 'OpenAI error' }) + "\n"); } catch {}
    res.end();
  }
});

app.listen(PORT, () => console.log(`API listening on http://0.0.0.0:${PORT}`));
